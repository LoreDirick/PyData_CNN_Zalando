{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zalando example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this all cover?\n",
    "**General (dense layers)**\n",
    "- Keras\n",
    "- Multilayer perceptron\n",
    "- activation functions\n",
    "- loss function\n",
    "- matrix notation practice\n",
    "- hyperparameter testing (trial and error)\n",
    "- regularization via early stopping (plot validation vs training accuracy)\n",
    "\n",
    "**CNN**\n",
    "- convolutional neural nets\n",
    "- use of all layers: convolution layers, pooling layers, Fully connected layers\n",
    "- sense of the general layer structure\n",
    "- show pictures in python\n",
    "- Visualizing intermediate convnet outputs \n",
    "- regularization via dropout\n",
    "- make a class prediction\n",
    "\n",
    "**Things I have not done (yet)**\n",
    "- k-fold cross-validation (yet wonder about the added value at this stage, makes comp time longer. Or should make the data smaller)\n",
    "- having jpeg images and actually making them manageable to work with (is this done earlier?)\n",
    "- data augmentation --> need de jpegs to do so\n",
    "- vanishing/exploding gradients\n",
    "- L2 regularization\n",
    "---> sequence processing with convolutional neural networks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we'll get a sense of just how powerful convolutional neural networks get! This data set, the so-called `Fashion-MNIST` was constructed by a research team at online fashion retailer Zalando.  Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image  as img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load mnist_reader.py\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = load_mnist( path = \"data_zalando/\",kind='train')\n",
    "test_images, test_labels = load_mnist(path = \"data_zalando/\", kind='t10k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fairly big data set. If we keep it as such, runtimes will be longer. Let's take samples of this data set, so we work with 10,000 training instances and 2,500 test instances. Do remember that working with bigger data is generally always better when it comes to training models and getting more accurate results, longer computation times are pretty much the only downside!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "import random\n",
    "index_train = range(0,59999)\n",
    "index_test= range(0,9999)\n",
    "\n",
    "random.seed(1109)\n",
    "train_sample = sample(index_train,  10000)\n",
    "test_sample = sample(index_test,  2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grayscale images: looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images[train_sample]\n",
    "train_labels = train_labels[train_sample]\n",
    "test_images = test_images[test_sample]\n",
    "test_labels = test_labels[test_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the shape of the train_images and test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entries for the first axis are the rows, the 2nd axis are the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 784)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that now, there are 10,000 observations in the training set, each representing 1 image of 28 x 28 pixels, so each observation contains 784 inputs that range between [0,255] (for a greyscale image, going from white to black). Similarly, there are 2500 images in the test set. Let's have a look at a random observation from `train_images`, let's say, the 4th observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   2,   1,   0,   1,   1,   1,   0, 146, 173,\n",
       "       138, 141, 146, 173,   0,   0,   3,   1,   1,   1,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   1,   3,   3,   0,   0,   0,  50,\n",
       "       231, 244, 248, 249, 249, 232, 148,   0,   0,   0,   0,   3,   1,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   2,   0,   0,  12,  97,\n",
       "       173, 213, 190, 228, 255, 245, 245, 186, 202, 176, 103,  39,   0,\n",
       "         0,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,  13, 177,\n",
       "       191, 203, 199, 197, 181, 186, 219, 225, 203, 169, 191, 198, 207,\n",
       "       196, 203,  65,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       168, 205, 186, 183, 183, 186, 199, 215, 198, 172, 217, 213, 190,\n",
       "       180, 181, 183, 194, 196,  43,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0, 113, 209, 184, 188, 188, 188, 188, 183, 185, 207, 148, 186,\n",
       "       181, 184, 186, 186, 184, 181, 192, 201,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0, 185, 201, 191, 190, 190, 189, 189, 187, 197, 195,\n",
       "       160, 192, 184, 185, 181, 185, 184, 190, 185, 208,  23,   0,   0,\n",
       "         0,   0,   0,   0,   3, 219, 194, 195, 192, 190, 190, 188, 188,\n",
       "       191, 202, 187, 189, 181, 186, 187, 179, 183, 180, 179, 207,  61,\n",
       "         0,   0,   0,   0,   0,   0,  52, 213, 192, 198, 196, 193, 191,\n",
       "       189, 187, 186, 189, 185, 189, 185, 186, 202, 181, 186, 179, 183,\n",
       "       205,  99,   0,   0,   0,   0,   0,   0,  88, 201, 188, 213, 220,\n",
       "       191, 193, 190, 189, 188, 187, 187, 188, 188, 185, 185, 179, 198,\n",
       "       217, 191, 189, 137,   0,   0,   0,   0,   0,   0, 179, 216, 224,\n",
       "       247, 208, 201, 201, 194, 192, 189, 189, 188, 189, 187, 191, 191,\n",
       "       189, 185, 237, 226, 208, 177,   0,   0,   0,   0,   0,   0,  51,\n",
       "       154, 208, 175,  63, 212, 202, 197, 193, 191, 189, 188, 189, 190,\n",
       "       190, 190, 202, 133,  87, 220, 140,  27,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  59, 211, 191, 201, 191, 191, 189, 188,\n",
       "       189, 188, 187, 181, 196, 162,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  82, 211, 192, 202, 191, 192,\n",
       "       190, 188, 189, 188, 190, 186, 188, 169,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   3,   0,  82, 201, 191, 201,\n",
       "       191, 192, 190, 188, 189, 188, 191, 191, 188, 164,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   5,   0,  70, 195,\n",
       "       190, 194, 189, 191, 190, 188, 189, 189, 185, 191, 188, 157,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   5,   0,\n",
       "        63, 195, 187, 191, 190, 191, 190, 189, 189, 190, 185, 187, 191,\n",
       "       151,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         5,   0,  66, 198, 186, 192, 190, 191, 190, 189, 189, 189, 189,\n",
       "       178, 195, 140,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   5,   0,  61, 195, 186, 192, 190, 191, 190, 189, 189,\n",
       "       190, 188, 178, 194, 135,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   4,   0,  68, 192, 186, 193, 191, 191, 191,\n",
       "       190, 189, 190, 188, 184, 191, 141,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   5,   0,  74, 193, 185, 194, 192,\n",
       "       192, 191, 190, 189, 189, 188, 185, 191, 151,   0,   0,   1,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   5,   0,  74, 192, 187,\n",
       "       194, 192, 192, 193, 192, 189, 187, 188, 185, 192, 165,   0,   0,\n",
       "         1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   5,   0,  74,\n",
       "       193, 189, 193, 192, 193, 193, 191, 189, 187, 188, 185, 191, 175,\n",
       "         0,   0,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   5,\n",
       "         0,  76, 197, 189, 190, 191, 193, 190, 188, 189, 190, 187, 184,\n",
       "       190, 185,   0,   0,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   5,   0,  77, 197, 189, 189, 191, 192, 190, 190, 189, 189,\n",
       "       187, 185, 188, 197,   0,   0,   2,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   5,   0,  75, 196, 186, 185, 186, 187, 186, 186,\n",
       "       186, 186, 181, 184, 185, 172,   0,   0,   3,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   6,   0,  88, 204, 191, 206, 205, 205,\n",
       "       205, 205, 205, 205, 204, 202, 190, 184,  14,   0,   4,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   4,   0,  63, 175, 138, 140,\n",
       "       140, 141, 137, 136, 136, 136, 132, 130, 132, 148,   4,   0,   2,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice if we could actually see what this picture looks like. Note that the form $(n\\_obs, total\\_n\\_pixels)$, which is what we obtained looking at `np.shape(train_images)`, is desirable when training neural networks, but if we want to *visualize* the actual images, we need to reshape $total\\_n\\_pixels$ to something of the form ($horiz\\_pixels$, $vertical\\_pixels$). Let's use the numpy function `matrix.reshape` to look at the 4th image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_example= np.matrix.reshape(train_images[3],28,28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly have a look at how this changed the data structure. Notice how you have an 28 x 28 matrix now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   2,   1,   0,   1,   1,   1,   0, 146, 173,\n",
       "        138, 141, 146, 173,   0,   0,   3,   1,   1,   1,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   1,   3,   3,   0,   0,   0,  50, 231, 244,\n",
       "        248, 249, 249, 232, 148,   0,   0,   0,   0,   3,   1,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   2,   0,   0,  12,  97, 173, 213, 190, 228,\n",
       "        255, 245, 245, 186, 202, 176, 103,  39,   0,   0,   2,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,  13, 177, 191, 203, 199, 197, 181, 186,\n",
       "        219, 225, 203, 169, 191, 198, 207, 196, 203,  65,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0, 168, 205, 186, 183, 183, 186, 199, 215,\n",
       "        198, 172, 217, 213, 190, 180, 181, 183, 194, 196,  43,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 113, 209, 184, 188, 188, 188, 188, 183, 185,\n",
       "        207, 148, 186, 181, 184, 186, 186, 184, 181, 192, 201,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 185, 201, 191, 190, 190, 189, 189, 187, 197,\n",
       "        195, 160, 192, 184, 185, 181, 185, 184, 190, 185, 208,  23,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   3, 219, 194, 195, 192, 190, 190, 188, 188, 191,\n",
       "        202, 187, 189, 181, 186, 187, 179, 183, 180, 179, 207,  61,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,  52, 213, 192, 198, 196, 193, 191, 189, 187, 186,\n",
       "        189, 185, 189, 185, 186, 202, 181, 186, 179, 183, 205,  99,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,  88, 201, 188, 213, 220, 191, 193, 190, 189, 188,\n",
       "        187, 187, 188, 188, 185, 185, 179, 198, 217, 191, 189, 137,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0, 179, 216, 224, 247, 208, 201, 201, 194, 192, 189,\n",
       "        189, 188, 189, 187, 191, 191, 189, 185, 237, 226, 208, 177,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,  51, 154, 208, 175,  63, 212, 202, 197, 193, 191,\n",
       "        189, 188, 189, 190, 190, 190, 202, 133,  87, 220, 140,  27,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  59, 211, 191, 201, 191, 191,\n",
       "        189, 188, 189, 188, 187, 181, 196, 162,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  82, 211, 192, 202, 191, 192,\n",
       "        190, 188, 189, 188, 190, 186, 188, 169,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   3,   0,  82, 201, 191, 201, 191, 192,\n",
       "        190, 188, 189, 188, 191, 191, 188, 164,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   5,   0,  70, 195, 190, 194, 189, 191,\n",
       "        190, 188, 189, 189, 185, 191, 188, 157,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   5,   0,  63, 195, 187, 191, 190, 191,\n",
       "        190, 189, 189, 190, 185, 187, 191, 151,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   5,   0,  66, 198, 186, 192, 190, 191,\n",
       "        190, 189, 189, 189, 189, 178, 195, 140,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   5,   0,  61, 195, 186, 192, 190, 191,\n",
       "        190, 189, 189, 190, 188, 178, 194, 135,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   4,   0,  68, 192, 186, 193, 191, 191,\n",
       "        191, 190, 189, 190, 188, 184, 191, 141,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   5,   0,  74, 193, 185, 194, 192, 192,\n",
       "        191, 190, 189, 189, 188, 185, 191, 151,   0,   0,   1,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   5,   0,  74, 192, 187, 194, 192, 192,\n",
       "        193, 192, 189, 187, 188, 185, 192, 165,   0,   0,   1,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   5,   0,  74, 193, 189, 193, 192, 193,\n",
       "        193, 191, 189, 187, 188, 185, 191, 175,   0,   0,   1,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   5,   0,  76, 197, 189, 190, 191, 193,\n",
       "        190, 188, 189, 190, 187, 184, 190, 185,   0,   0,   1,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   5,   0,  77, 197, 189, 189, 191, 192,\n",
       "        190, 190, 189, 189, 187, 185, 188, 197,   0,   0,   2,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   5,   0,  75, 196, 186, 185, 186, 187,\n",
       "        186, 186, 186, 186, 181, 184, 185, 172,   0,   0,   3,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   6,   0,  88, 204, 191, 206, 205, 205,\n",
       "        205, 205, 205, 205, 204, 202, 190, 184,  14,   0,   4,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   4,   0,  63, 175, 138, 140, 140, 141,\n",
       "        137, 136, 136, 136, 132, 130, 132, 148,   4,   0,   2,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182d4225c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(reshape_example, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the resolution of this image is pretty low, but for this example it's definitely clear that this is a shirt!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the labels. As mentioned, there are 10 classes of clothing in this data set. In the `train_labels` (`test_labels`) data set, this means that there are exactly 10,000 (2500) inputs with each input ranging from [0,9]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 2, ..., 8, 1, 0], dtype=uint8)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we'll keep this decoded version to train and test our model (actually, we'll decode even further and perform one-hot encoding), it would be nice if we could do a sanity check on our data now and verify that the train instances are labeled correctly. Going back to the github repository, we find a table with information on what number is associated with which type of clothing (https://github.com/zalandoresearch/fashion-mnist#labels). \n",
    "\n",
    "Let's now create a dictionary to encode the data back from number to clothing category. The `encoded_train` and `encoded_test` objects now contains the encoded labels. Let's have a look at the 4th observation in the `encoded_train` and check if the picture we showed before is indeed identified as a t-shirt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = {0:'T-shirt/Top',\n",
    "                  1:'Trouser',\n",
    "                  2:'Pullover',\n",
    "                  3:'Dress',\n",
    "                  4:'Coat',\n",
    "                  5:'Sandal',\n",
    "                  6:'Shirt',\n",
    "                  7:'Sneaker',\n",
    "                  8:'Bag',\n",
    "                  9:'Ankle Boot'}\n",
    "\n",
    "encoded_train = [encoded_labels[i] for i in train_labels]\n",
    "encoded_test = [encoded_labels[i] for i in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T-shirt/Top'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This label seems to be correct! By replacing the value passed in `index_test` below, you can look at several pictures and the associated label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182da2df60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Pullover'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_fill = 233 # replace with any value between 0 and 9999\n",
    "\n",
    "reshape= np.matrix.reshape(train_images[index_fill],28,28)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(reshape, cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "encoded_train[index_fill]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build densely connected network as a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time you run a newer, more advanced model on your data, you would want to make sure you compare it with what you have learned. For that reason, let's look at a dense neural network as a baseline before diving into convolutional networks. As data is manipulated slightly differently in convolutional networks, let's perform data manipulation for the dense network, and we'll start over from scratch for the convolutional neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the image data is stored in an array of shape (10000, 784) of type uint8 with values in the [0, 255] interval. To serve as input for the model, we want transform it into a float32 array of the same shape, but with values between 0 and 1 instead of 0 and 255.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not quite done yet. Remember that instead of a label somewhere [0,9], neural networks generally use one-hot encoding. For this exercise, this means that the label for each observation is replaced by a vector containing 9 `0`'s and just 1 `1`, in the place of the label index.\n",
    "\n",
    "You can use `to_categorical` in `keras.utils` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels= to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 10)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our 10,000 training set elements, let set 2000 aside for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_val = train_images[:2000]\n",
    "partial_img_train  = train_images[2000:]\n",
    "label_val = train_labels[:2000]\n",
    "partial_label_train = train_labels[2000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build our baseline network. You'll build two dense hidden layers, the first one with 128 hidden units, and the second one with 64 hidden units. Use the rectified linear unit as an activation function for both. Remember that the input shape is 28 x 28 for both layers. The last layer of the network should have a softmax activation function (because this is a multiclass problem), and 10 units.\n",
    "\n",
    "Let's train the model for 50 epochs in batches of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 0s 49us/step - loss: 0.9680 - acc: 0.6643 - val_loss: 0.7391 - val_acc: 0.7330\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.6132 - acc: 0.7807 - val_loss: 0.6187 - val_acc: 0.7780\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.5347 - acc: 0.8123 - val_loss: 0.4792 - val_acc: 0.8365\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.4916 - acc: 0.8192 - val_loss: 0.7052 - val_acc: 0.7500\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.4606 - acc: 0.8365 - val_loss: 0.4932 - val_acc: 0.8295\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.4223 - acc: 0.8465 - val_loss: 0.4650 - val_acc: 0.8360\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.4046 - acc: 0.8531 - val_loss: 0.5430 - val_acc: 0.8045\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.3874 - acc: 0.8599 - val_loss: 0.5632 - val_acc: 0.7905\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.3684 - acc: 0.8616 - val_loss: 0.4702 - val_acc: 0.8365\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.3514 - acc: 0.8720 - val_loss: 0.5690 - val_acc: 0.8030\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.3386 - acc: 0.8768 - val_loss: 0.5713 - val_acc: 0.8045\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.3215 - acc: 0.8817 - val_loss: 0.4235 - val_acc: 0.8620\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.3182 - acc: 0.8861 - val_loss: 0.5728 - val_acc: 0.7925\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.2937 - acc: 0.8936 - val_loss: 0.4936 - val_acc: 0.8435\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.2879 - acc: 0.8958 - val_loss: 0.4394 - val_acc: 0.8420\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.2780 - acc: 0.8981 - val_loss: 0.4258 - val_acc: 0.8505\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.2621 - acc: 0.9039 - val_loss: 0.4085 - val_acc: 0.8615\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 0s 20us/step - loss: 0.2594 - acc: 0.9042 - val_loss: 0.4441 - val_acc: 0.8465\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.2501 - acc: 0.9103 - val_loss: 0.4181 - val_acc: 0.8495\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 0.2404 - acc: 0.9126 - val_loss: 0.3946 - val_acc: 0.8655\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 0.2291 - acc: 0.9159 - val_loss: 0.4560 - val_acc: 0.8535\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 0s 20us/step - loss: 0.2296 - acc: 0.9167 - val_loss: 0.5189 - val_acc: 0.8415\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.2183 - acc: 0.9174 - val_loss: 0.5194 - val_acc: 0.8215\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.2092 - acc: 0.9254 - val_loss: 0.4129 - val_acc: 0.8590\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.2053 - acc: 0.9250 - val_loss: 0.4639 - val_acc: 0.8430\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.1983 - acc: 0.9278 - val_loss: 0.4966 - val_acc: 0.8330\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1899 - acc: 0.9289 - val_loss: 0.4480 - val_acc: 0.8530\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 0s 22us/step - loss: 0.1777 - acc: 0.9334 - val_loss: 0.4504 - val_acc: 0.8580\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 0s 21us/step - loss: 0.1832 - acc: 0.9329 - val_loss: 0.4487 - val_acc: 0.8530\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 0.1725 - acc: 0.9390 - val_loss: 0.4230 - val_acc: 0.8605\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 0.1637 - acc: 0.9397 - val_loss: 0.4623 - val_acc: 0.8485\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 0s 19us/step - loss: 0.1626 - acc: 0.9397 - val_loss: 0.4419 - val_acc: 0.8520\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 0s 18us/step - loss: 0.1568 - acc: 0.9426 - val_loss: 0.4768 - val_acc: 0.8545\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1447 - acc: 0.9464 - val_loss: 0.4404 - val_acc: 0.8660\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1466 - acc: 0.9475 - val_loss: 0.4836 - val_acc: 0.8490\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.1403 - acc: 0.9486 - val_loss: 0.4449 - val_acc: 0.8650\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 0s 37us/step - loss: 0.1369 - acc: 0.9486 - val_loss: 0.4619 - val_acc: 0.8620\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.1330 - acc: 0.9534 - val_loss: 0.5691 - val_acc: 0.8285\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 0s 23us/step - loss: 0.1345 - acc: 0.9504 - val_loss: 0.5617 - val_acc: 0.8425\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.1282 - acc: 0.9547 - val_loss: 0.4673 - val_acc: 0.8710\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.1179 - acc: 0.9571 - val_loss: 0.4879 - val_acc: 0.8500\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 0s 34us/step - loss: 0.1117 - acc: 0.9589 - val_loss: 0.5136 - val_acc: 0.8565\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.1160 - acc: 0.9574 - val_loss: 0.5140 - val_acc: 0.8560\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 0s 38us/step - loss: 0.1099 - acc: 0.9622 - val_loss: 0.5980 - val_acc: 0.8385\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.1083 - acc: 0.9597 - val_loss: 0.5255 - val_acc: 0.8580\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.1034 - acc: 0.9631 - val_loss: 0.5501 - val_acc: 0.8645\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.1009 - acc: 0.9641 - val_loss: 0.7539 - val_acc: 0.8095\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.1032 - acc: 0.9625 - val_loss: 0.6644 - val_acc: 0.8495\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.0957 - acc: 0.9663 - val_loss: 0.5288 - val_acc: 0.8625\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.0934 - acc: 0.9666 - val_loss: 0.5594 - val_acc: 0.8515\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "dense = models.Sequential()\n",
    "dense.add(layers.Dense(128, activation='relu', input_shape=(28 * 28,)))\n",
    "dense.add(layers.Dense(64, activation='relu', input_shape=(28 * 28,)))\n",
    "dense.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "dense.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "dense_fit = dense.fit(partial_img_train, \n",
    "                      partial_label_train, \n",
    "                      epochs=50, \n",
    "                      batch_size=128,\n",
    "                      validation_data=(img_val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the evolution of the training versus validation loss along the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18316c5710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_dense = dense_fit.history\n",
    "loss_values = hist_dense['loss']\n",
    "val_loss_values = hist_dense['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g.', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g', label='Validation loss')\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the evolution of the training versus validation accuracy along the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_values = hist_dense['acc'] \n",
    "val_acc_values = hist_dense['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r.', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy and loss seem to be fairly volatile, but as we move along the epochs, the validation loss seems to go up again.\n",
    "It seems appropriate to stop training after about ~20 epochs. Let's run the model again changing `epochs = 20`, and let's look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_final = models.Sequential()\n",
    "dense_final.add(layers.Dense(128, activation='relu', input_shape=(28 * 28,)))\n",
    "dense_final.add(layers.Dense(64, activation='relu', input_shape=(28 * 28,)))\n",
    "dense_final.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "dense_final.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "dense_final_fit = dense_final.fit(partial_img_train, \n",
    "                                  partial_label_train, \n",
    "                                  epochs=20, \n",
    "                                  batch_size=128,\n",
    "                                  validation_data=(img_val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dense_final = dense_final.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dense_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set classification accuracy is about 80%. Not bad, for a 10-fold classification and a fairly small data size. Let's see if we can do better with a convolutional net!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. A convolutional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data needs to be pre-processed differently. Let's start again from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = load_mnist( path = \"data_zalando/\",kind='train')\n",
    "test_images, test_labels = load_mnist(path = \"data_zalando/\", kind='t10k')\n",
    "\n",
    "import random\n",
    "index_train = range(0,59999)\n",
    "index_test= range(0,9999)\n",
    "\n",
    "random.seed(1109)\n",
    "train_sample = sample(index_train,  10000)\n",
    "test_sample = sample(index_test,  2500)\n",
    "\n",
    "train_images = train_images[train_sample]\n",
    "train_labels = train_labels[train_sample]\n",
    "test_images = test_images[test_sample]\n",
    "test_labels = test_labels[test_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the input layer for a dense neural network is fine to be $(n\\_obs, total\\_n\\_pixels)$, we want the training in data in convolutions to be $(n\\_obs, horiz\\_pixels, vertical\\_pixels, num\\_channels)$, because convolutions operate over 3D tensors, with two spatial axes (height and width) and a depth axis (also referred to as $channels$ axis). For an RGB image, the dimension of the depth axis is 3, because of the three color channels red, green, and blue. For a black-and-white picture the depth is 1 (levels of gray). We need to reshape train_images and test_images as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.reshape((2500, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the image data is stored in an array of shape (10000, 784) of type uint8 with values in the [0, 255] interval. To serve as input for the convolutional model, you'll need to transform it again into a float32 array of the same shape, but with values between 0 and 1 instead of 0 and 255.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = test_images.astype('float32') / 255\n",
    "train_images = train_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform one-hot encoding on the labels again! You can use `to_categorical` in `keras.utils` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll want to split the training set up in actual training data and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_val = train_images[:2000]\n",
    "partial_img_train  = train_images[2000:]\n",
    "label_val = train_labels[:2000]\n",
    "partial_label_train = train_labels[2000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at exactly what convolutions do. A convolution starts by sliding \"patches\" over the input. Patches are typically 3 x 3 or 5 x 5. Because of patching, the output width and height will differ from the input and depend on 2 things:\n",
    "- border effects\n",
    "- The use of strides\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input shape for the first layer is (28, 28, 1), as previously discussed. For a convolutional layer, 2 additional decisions need to be made:\n",
    "- The depth of the convolutional layer, also the number of filters created by the convolution.\n",
    "- The dimensions of the so-called \"patches\" (generally 3 x 3 or 5 x 5).\n",
    "\n",
    "Using keras, this can be programmed as follows:\n",
    "`Conv2D(output_depth, (window_height, window_width))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Running the first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 3 convolutional layers (the first one with output depth 32, the latter two with output depth 64). Make sure all layers use 3x3 patches.\n",
    "\n",
    "Then, to downsample feature maps use maxpooling layers in between the convolution layers. Use the common 2 Ã— 2 window and stride 2, in order to downsample the feature maps by a factor of 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small convnet\n",
    "from keras import layers \n",
    "from keras import models\n",
    "\n",
    "conv = models.Sequential()\n",
    "conv.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) \n",
    "conv.add(layers.MaxPooling2D((2, 2)))\n",
    "conv.add(layers.Conv2D(64, (3, 3), activation='relu')) \n",
    "conv.add(layers.MaxPooling2D((2, 2)))\n",
    "conv.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# you can use `padding = valid` and `padding = same`. Strides \\= 1 for convolutional layers are not very common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When convolutional networks, generally, dense layers are added towards the end of the network. First, we need to flatten the 3D outputs to 1D, and then we can add a few dense layers. Let's add one hidden layer with 64 units and relu, and again a softmax layer towards the end of the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.add(layers.Flatten())\n",
    "conv.add(layers.Dense(64, activation='relu'))\n",
    "conv.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the output layers change dimension. A summary of what exaclty is happening can be found using `.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this model, using 30 epochs and a batch size of 64. Use both the train data and the validation data in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "conv_fit = conv.fit(partial_img_train, \n",
    "                      partial_label_train, \n",
    "                      epochs=30, \n",
    "                      batch_size=64,\n",
    "                      validation_data=(img_val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the evolution of the training versus validation loss along the epochs again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "hist_conv = conv_fit.history\n",
    "loss_values = hist_conv['loss']\n",
    "val_loss_values = hist_conv['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g.', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g', label='Validation loss')\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the evolution of the training versus validation loss along the epochs again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = hist_conv['acc'] \n",
    "val_acc_values = hist_conv['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r.', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy and loss seem to be fairly volatile again, but as we move along the epochs, the validation loss definitely goes up. Here, seems appropriate to stop after about ~14 epochs. Let's retrain the entire model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_final = models.Sequential()\n",
    "conv_final.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) \n",
    "conv_final.add(layers.MaxPooling2D((2, 2)))\n",
    "conv_final.add(layers.Conv2D(64, (3, 3), activation='relu')) \n",
    "conv_final.add(layers.MaxPooling2D((2, 2)))\n",
    "conv_final.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "conv_final.add(layers.Flatten())\n",
    "conv_final.add(layers.Dense(64, activation='relu'))\n",
    "conv_final.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "conv_final.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "conv_final_fit = conv_final.fit(partial_img_train, \n",
    "                      partial_label_train, \n",
    "                      epochs=14, \n",
    "                      batch_size=64,\n",
    "                      validation_data=(img_val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_conv_final = conv_final.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_conv_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set classification accuracy is 86.3%. Coming from 83.6 in the sequential model this is about a 3% improvement. For bigger data sets this could even be more! Let's see if we can still improve our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Using drop-out regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we \"early stopping\" to fight overfitted models. Recall that there is another method, dropout regularization, to counter overfitting. Rerun our `conv` model, yet with a dropout layer right before the densely connected classifier, with parameter 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_drop = models.Sequential()\n",
    "conv_drop.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) \n",
    "conv_drop.add(layers.MaxPooling2D((2, 2)))\n",
    "conv_drop.add(layers.Conv2D(64, (3, 3), activation='relu')) \n",
    "conv_drop.add(layers.MaxPooling2D((2, 2)))\n",
    "conv_drop.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "conv_drop.add(layers.Flatten())\n",
    "conv_drop.add(layers.Dropout(0.5))\n",
    "conv_drop.add(layers.Dense(64, activation='relu'))\n",
    "conv_drop.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "conv_drop.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "conv_drop_fit = conv_drop.fit(partial_img_train, \n",
    "                      partial_label_train, \n",
    "                      epochs=30, \n",
    "                      batch_size=64,\n",
    "                      validation_data=(img_val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "hist_conv_drop = conv_drop_fit.history\n",
    "loss_values = hist_conv_drop['loss']\n",
    "val_loss_values = hist_conv_drop['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g.', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'g', label='Validation loss')\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = hist_conv_drop['acc'] \n",
    "val_acc_values = hist_conv_drop['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r.', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the dropout regularization made the curves much smoother! Additionally, the respective accuracies and losses don't diverge to the extent they did before. Let's stop after 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_drop_final = models.Sequential()\n",
    "conv_drop_final.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) \n",
    "conv_drop_final.add(layers.MaxPooling2D((2, 2)))\n",
    "conv_drop_final.add(layers.Conv2D(64, (3, 3), activation='relu')) \n",
    "conv_drop_final.add(layers.MaxPooling2D((2, 2)))\n",
    "conv_drop_final.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "conv_drop_final.add(layers.Flatten())\n",
    "conv_drop_final.add(layers.Dropout(0.5))\n",
    "conv_drop_final.add(layers.Dense(64, activation='relu'))\n",
    "conv_drop_final.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "conv_drop_final.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "conv_drop_final_fit = conv_drop_final.fit(partial_img_train, \n",
    "                      partial_label_train, \n",
    "                      epochs=20, \n",
    "                      batch_size=64,\n",
    "                      validation_data=(img_val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = conv_drop_final.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at this! You were able to increase the test set accuracy by almost 1% (being 86.3% for the model without dropout regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = conv_drop_final.predict(test_images)\n",
    "index_test = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[index_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[index_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape= np.matrix.reshape(test_images[index_test],28,28)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(reshape, cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "encoded_test[index_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This coat is incorrectly classified as a dress! Not really surprising, looking at it, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Try tweaking the model yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The possibilities are literally endless!\n",
    "- add/remove layers\n",
    "- In some layers, change `padding = valid` and `padding = same`\n",
    "- change activation functions\n",
    "- change optimizer\n",
    "- change batch size\n",
    "- change patch dimensions (from 3 x 3 to 5 x 5)\n",
    "- If you have more time, try running everything on the bigger training set! Do results improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Visualizations (WIP, maybe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_fill = 1219 # replace by any value between 0 and 9999\n",
    "\n",
    "reshape= np.matrix.reshape(train_images[index_fill],28,28)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(reshape, cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "encoded_train[index_fill]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "layer_outputs = [layer.output for layer in conv_final.layers[:8]] \n",
    "activation_model = models.Model(inputs=conv_final.input, outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interm_tensor = np.expand_dims(train_images[index_test], axis=0) \n",
    "interm_tensor /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interm_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = activation_model.predict(interm_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_activation = activations[0]\n",
    "print(first_layer_activation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(first_layer_activation[0, :, :, 1], cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(first_layer_activation[0, :, :, 2], cmap='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
